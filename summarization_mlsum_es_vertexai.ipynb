{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c753f7bf-8e8e-4905-8043-8eb86e7aa770",
   "metadata": {},
   "source": [
    "# Training a PyTorch Text Summarization Model on [Vertex AI](https://cloud.google.com/vertex-ai) using a Huggingface model\n",
    "## Fine-tuning pre-trained [mT5](https://huggingface.co/google/mt5-small) model for a text summarization task in **Spanish**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f973a-477c-42bc-a2b1-57447c99603f",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "We will be fine-tuning a **`mBART`** (pre-trained) and **`mT5`** model for text summarization task in spanish.\n",
    "You can find the details about this model at [Hugging Face Hub](https://huggingface.co/bert-base-cased).\n",
    "\n",
    "For more notebooks with the state of the art PyTorch/Tensorflow/JAX, you can explore [Hugging FaceNotebooks](https://huggingface.co/transformers/notebooks.html).\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We will be using [MLSUM Dataset](https://https://huggingface.co/datasets/mlsum) from [Hugging Face Datasets](https://huggingface.co/datasets).\n",
    "\n",
    "MLSUM is the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset\n",
    "\n",
    "### Objective\n",
    "\n",
    "How to **Build, Train and Tune a PyTorch model on [Vertex AI](https://cloud.google.com/vertex-ai)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559bb9b-85f1-4c88-baef-041064c7236c",
   "metadata": {},
   "source": [
    "### Install additional packages\n",
    "\n",
    "**If your are not going to run the code locally you do not need to install these packages**\n",
    "\n",
    "Python dependencies required for this notebook are [Transformers](https://pypi.org/project/transformers/), [Datasets](https://pypi.org/project/datasets/) and [hypertune](https://github.com/GoogleCloudPlatform/cloudml-hypertune) will be installed in the Notebooks instance itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189df6c0-23f9-48ab-8a17-575a85c52a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710721d7-3eab-4162-b0f2-de6f1cd06fc2",
   "metadata": {},
   "source": [
    "If you want to run the training script locally run the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ed6bc-2b84-4967-86bc-ea865eecf89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install {USER_FLAG} --upgrade transformers\n",
    "!pip -q install {USER_FLAG} --upgrade datasets\n",
    "!pip -q install {USER_FLAG} --upgrade tqdm\n",
    "!pip -q install {USER_FLAG} --upgrade cloudml-hypertune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da69ce0-d5ff-4b9b-a215-d64c6043abff",
   "metadata": {},
   "source": [
    "We will be using [Vertex AI SDK for Python](https://cloud.google.com/vertex-ai/docs/start/client-libraries#python) to interact with Vertex AI services. The high-level `aiplatform` library is designed to simplify common data science workflows by using wrapper classes and opinionated defaults. \n",
    "\n",
    "#### Install Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748c364a-3a7a-4958-a532-8d61fde39e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install {USER_FLAG} --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b2a02-d767-4dde-b083-86f02f730b0c",
   "metadata": {
    "id": "f361541eff05"
   },
   "source": [
    "### Restart the Kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09539e4f-edca-4a48-b45c-2d25e67c037d",
   "metadata": {
    "id": "2d77a223d63d"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f257e-cb0a-4967-b87f-d7b20ecc6fc1",
   "metadata": {
    "id": "9bebb3b46278"
   },
   "source": [
    "## Prepare the enviroment and GCP elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca62355-2f39-4309-8d5c-1957b764ef5f",
   "metadata": {
    "id": "e1e4d8f0c294"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). \n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "1. Enable following APIs in your project required for running the tutorial\n",
    "    - [Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
    "    - [Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com)\n",
    "    - [Container Registry API](https://console.cloud.google.com/flows/enableapi?apiid=containerregistry.googleapis.com)\n",
    "    - [Cloud Build API](https://console.cloud.google.com/flows/enableapi?apiid=cloudbuild.googleapis.com)\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f54957-fd62-4d5d-aa9e-3a7f52cc978c",
   "metadata": {
    "id": "36a4450b7c2e"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud` or `google.auth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5af590-be93-4c42-a273-132bd82fad57",
   "metadata": {
    "id": "019e546007a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  gcp-ml-projects\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"gcp-ml-project\"  # <---CHANGE THIS TO YOUR PROJECT\n",
    "\n",
    "import os\n",
    "\n",
    "# Get your Google Cloud project ID using google.auth\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    import google.auth\n",
    "\n",
    "    _, PROJECT_ID = google.auth.default()\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "\n",
    "# validate PROJECT_ID\n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"gcp-ml-project\":\n",
    "    print(\n",
    "        f\"Please set your project id before proceeding to next step. Currently it's set as {PROJECT_ID}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c7d4d7-9891-4dc7-a612-fb7ed88e1e70",
   "metadata": {
    "id": "0c9906f72b18"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c384ca-021b-4148-989d-3bb058c5743a",
   "metadata": {
    "id": "e90182316f63"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_timestamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e394a-0bc1-4ed3-b599-9b142fc847af",
   "metadata": {
    "id": "4d96aec55eba"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "If you are using Google Cloud Notebooks, your environment is already authenticated. Else you need to authenticate to your GCP account. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53b0aa-6be2-4fe7-9ed0-9c9ec5f55cc1",
   "metadata": {
    "id": "35094e21d888"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a training job using the Cloud SDK, you upload a Python package containing your training code to a Cloud Storage bucket. Vertex AI runs the code from this package. In this tutorial, Vertex AI also saves the trained model that results from your job in the same bucket. Using this model artifact, you can then create Vertex AI model and endpoint resources in order to serve online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4dc1f4b-55de-4082-9775-1af97c93f8d6",
   "metadata": {
    "id": "e07102312039"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://gcp-ml-projects-bucket\"  # <---CHANGE THIS TO YOUR BUCKET\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02874a97-f875-45b2-8759-7087b53b7409",
   "metadata": {
    "id": "4908f26b84be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID = gcp-ml-projects\n",
      "BUCKET_NAME = gs://gcp-ml-projects-bucket\n",
      "REGION = us-central1\n"
     ]
    }
   ],
   "source": [
    "print(f\"PROJECT_ID = {PROJECT_ID}\")\n",
    "print(f\"BUCKET_NAME = {BUCKET_NAME}\")\n",
    "print(f\"REGION = {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2082c0-3922-480f-a9fa-b3ee3102d0d1",
   "metadata": {
    "id": "6a3aae29644f"
   },
   "source": [
    "---\n",
    "\n",
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d5cdf0-178e-4766-85d5-c49f230ab4b8",
   "metadata": {
    "id": "25f9882bab87"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fffda7-16ca-4b2d-9a0a-aa8a4b8d9c96",
   "metadata": {
    "id": "2586e4ed72ad"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b02067-cd92-49f3-9ac9-d106dba971ee",
   "metadata": {
    "id": "315724257beb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://gcp-ml-projects-bucket/aiplatform-custom-job-2022-03-10-15:53:48.692/\n",
      "                                 gs://gcp-ml-projects-bucket/aiplatform-custom-job-2022-03-10-18:42:21.830/\n",
      "                                 gs://gcp-ml-projects-bucket/aiplatform-custom-job-2022-03-11-08:17:57.154/\n",
      "                                 gs://gcp-ml-projects-bucket/aiplatform-custom-training-2022-03-06-20:52:30.161/\n",
      "                                 gs://gcp-ml-projects-bucket/aiplatform-custom-training-2022-03-07-18:01:30.625/\n",
      "                                 gs://gcp-ml-projects-bucket/aiplatform-custom-training-2022-03-08-08:00:26.038/\n",
      "                                 gs://gcp-ml-projects-bucket/aiplatform-custom-training-2022-03-10-15:24:33.751/\n",
      "                                 gs://gcp-ml-projects-bucket/aiplatform-custom-training-2022-03-11-23:00:26.528/\n",
      "                                 gs://gcp-ml-projects-bucket/datasets/\n",
      "                                 gs://gcp-ml-projects-bucket/mpg/\n",
      "                                 gs://gcp-ml-projects-bucket/pytorch-on-gcp/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80477e9-a891-40a2-8324-6e91e9f58082",
   "metadata": {
    "id": "276e063f41eb"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b089a181-0000-42f1-85f1-0cfae864a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0151998-f2c5-40e2-ae0a-39115ff85503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f1717e8-fca9-4593-91ef-71af48e6145b",
   "metadata": {
    "id": "9805f4ffaa4d"
   },
   "outputs": [],
   "source": [
    "# Set a global varibable to identify our experiment\n",
    "APP_NAME = \"mt5-small-summ-mlsum-es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9965495-dea9-4ee7-a89e-acae743273f3",
   "metadata": {
    "id": "4c7643572342"
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a396df7-883d-4753-bf6c-f511af72a73b",
   "metadata": {
    "id": "46a8d95366ee"
   },
   "source": [
    "## Training on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad6991-5a41-4b46-9ada-e66aef426d57",
   "metadata": {
    "id": "e97e8af2376c"
   },
   "source": [
    "You can do local experimentation on your Notebooks instance. However, for larger datasets or models often a vertically scaled compute or horizontally distributed training is required. The most effective way to perform this task is to leverage [Vertex AI custom training service](https://cloud.google.com/vertex-ai/docs/training/custom-training) for following reasons:\n",
    "\n",
    "- **Automatically provision and de-provision resources**: Training job on Vertex AI will automatically provision computing resources, performs the training task and ensures deletion of compute resources once the training job is finished.\n",
    "- **Reusability and portability**: You can package training code with its parameters and dependencies into a container and create a portable component. This container can then be run with different scenarios such as hyperparameter tuning, different data sources and more.\n",
    "- **Training at scale**: You can run a [distributed training job](https://cloud.google.com/vertex-ai/docs/training/distributed-training) with AI allowing you to train models in a cluster across multiple nodes in parallel and resulting in faster training time. \n",
    "- **Logging and Monitoring**: The training service logs messages from the job to [Cloud Logging](https://cloud.google.com/logging/docs) and can be monitored while the job is running.\n",
    "\n",
    "In this part of the notebook, we show how to scale the training job with Vertex AI by packaging the code and create a training pipeline to orchestrate a training job. There are three steps to run a training job using [Vertex AI custom training service](https://cloud.google.com/vertex-ai/docs/training/custom-training):\n",
    "\n",
    "- **STEP 1**: Determine training code structure - Packaging as a Python source distribution or as a custom container image\n",
    "- **STEP 2**: Chose a custom training method - custom job, hyperparameter training job or training pipeline\n",
    "- **STEP 3**: Run the training job\n",
    "\n",
    "\n",
    "#### Custom training methods\n",
    "\n",
    "There are three types of Vertex AI resources you can create to train custom models on Vertex AI:\n",
    "\n",
    "- **[Custom jobs](https://cloud.google.com/vertex-ai/docs/training/create-custom-job):** With a custom job you configure the settings to run your training code on Vertex AI such as worker pool specs - machine types, accelerators, Python training spec or custom container spec. \n",
    "- **[Hyperparameter tuning jobs](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning):** Hyperparameter tuning jobs automate tuning of hyperparameters of your model based on the criteria you configure such as goal/metric to optimize, hyperparameters values and number of trials to run.\n",
    "- **[Training pipelines](https://cloud.google.com/vertex-ai/docs/training/create-training-pipeline):** Orchestrates custom training jobs or hyperparameter tuning jobs with additional steps after the training job is successfully completed.\n",
    "\n",
    "Please refer to the [documentation](https://cloud.google.com/vertex-ai/docs/training/custom-training-methods) for further details.\n",
    "\n",
    "In this notebook, we will cover Custom Jobs and Hyperparameter tuning jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52a7184-059a-41d2-9662-1d7afa6a22f6",
   "metadata": {
    "id": "585ba542aa40"
   },
   "source": [
    "### Packaging the training application\n",
    "\n",
    "Before running the training job on Vertex AI, the training application code and any dependencies must be packaged and uploaded to Cloud Storage bucket or Container Registry or Artifact Registry that your Google Cloud project can access. This sections shows how to package and stage your application in the cloud.\n",
    "\n",
    "There are two ways to package your application and dependencies and train on Vertex AI:\n",
    "\n",
    "1. [Create a Python source distribution](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container) with the training code and dependencies to use with a [pre-built containers](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) on Vertex AI\n",
    "2. Use [custom containers](https://cloud.google.com/ai-platform/training/docs/custom-containers-training) to package dependencies using Docker containers\n",
    "\n",
    "## SELECT THER MAIN OPTION\n",
    "**This notebook shows both packaging options to run a custom training job on Vertex AI.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b290920f-54bb-4847-8a3c-7ea46241db96",
   "metadata": {
    "id": "4b8b5333b577"
   },
   "source": [
    "#### Recommended Training Application Structure\n",
    "\n",
    "You can structure your training application in any way you like. However, the [following structure](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container#structure) is commonly used in Vertex AI samples, and having your project's organization be similar to the samples can make it easier for you to follow the samples.\n",
    "\n",
    "We have two directories `python_package` and `custom_container` showing both the packaging approaches. `README.md` files inside each directory has details on the directory structure and instructions on how to run application locally and on the cloud.\n",
    "\n",
    "```\n",
    ".\n",
    "├── custom_container\n",
    "│   ├── Dockerfile\n",
    "│   ├── README.md\n",
    "│   ├── scripts\n",
    "│   │   └── train-cloud.sh\n",
    "│   └── trainer -> ../python_package/trainer/\n",
    "├── python_package\n",
    "│   ├── README.md\n",
    "│   ├── scripts\n",
    "│   │   └── train-cloud.sh\n",
    "│   ├── setup.py\n",
    "│   └── trainer\n",
    "│       ├── __init__.py\n",
    "│       ├── experiment.py\n",
    "│       ├── metadata.py\n",
    "│       ├── model.py\n",
    "│       ├── task.py\n",
    "│       └── utils.py\n",
    "└── pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb    --> This notebook\n",
    "```\n",
    "\n",
    "1. Main project directory contains your `setup.py` file or `Dockerfile` with the dependencies. \n",
    "2. Use a subdirectory named `trainer` to store your main application module and `scripts` to submit training jobs locally or cloud\n",
    "3. Inside `trainer` directory:\n",
    "    - `task.py` - Main application module 1) initializes and parse task arguments (hyper parameters), and 2) entry point to the trainer\n",
    "    - `experiment.py` - Runs the model training and evaluation experiment, and exports the final model.\n",
    "    - `utils.py` - Includes utility functions such as data input functions to read data, save model to GCS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b5c003-bf62-4da5-a20b-c731e49eec60",
   "metadata": {
    "id": "f7466d414a0e"
   },
   "source": [
    "### Run Custom Job on Vertex AI Training with a pre-built container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7730093-2909-490e-a7fb-64ef0098410e",
   "metadata": {
    "id": "1a675e6ee9dc"
   },
   "source": [
    "Vertex AI provides Docker container images that can be run as [pre-built containers](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers#available_container_images) for custom training. These containers include common dependencies used in training code based on the Machine Learning framework and framework version.\n",
    "\n",
    "In this notebook, we are using Hugging Face Datasets and fine tuning a transformer model from Hugging Face Transformers Library for text summarization task using PyTorch. We will use [pre-built container for PyTorch](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers#pytorch) and package the training application code by adding standard Python dependencies - `transformers`, `datasets`, `nltk` and `rouge_score` - in the `setup.py` file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e33e75-63af-4e4e-9d21-6f0f938c724b",
   "metadata": {
    "id": "9481da82c2ca"
   },
   "source": [
    "Initialize the variables to define pre-built container image, location of training application and training module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c1375d3-70be-4a21-8153-91865e09f0ca",
   "metadata": {
    "id": "100eac26f547"
   },
   "outputs": [],
   "source": [
    "PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-7:latest\"\n",
    ")\n",
    "\n",
    "PYTHON_PACKAGE_APPLICATION_DIR = \"python_package\"\n",
    "\n",
    "source_package_file_name = f\"{PYTHON_PACKAGE_APPLICATION_DIR}/dist/trainer-0.1.tar.gz\"\n",
    "python_package_gcs_uri = (\n",
    "    f\"{BUCKET_NAME}/pytorch-on-gcp/{APP_NAME}/train/python_package/trainer-0.1.tar.gz\"\n",
    ")\n",
    "python_module_name = \"trainer.task\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c18994-72eb-4878-b940-a5deb39abea2",
   "metadata": {
    "id": "a90b5019436d"
   },
   "source": [
    "Run the following command to create a source distribution, dist/trainer-0.1.tar.gz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c28d99b0-12ef-4df7-b3df-5ebbc45474f3",
   "metadata": {
    "id": "9fe9d30e9806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying README.md -> trainer-0.1\n",
      "copying setup.py -> trainer-0.1\n",
      "copying trainer/__init__.py -> trainer-0.1/trainer\n",
      "copying trainer/experiment.py -> trainer-0.1/trainer\n",
      "copying trainer/task.py -> trainer-0.1/trainer\n",
      "copying trainer/utils.py -> trainer-0.1/trainer\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/requires.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!cd {PYTHON_PACKAGE_APPLICATION_DIR} && python3 setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a07ae-3116-4634-a77a-0a5a0a08b9d1",
   "metadata": {
    "id": "7b769681d231"
   },
   "source": [
    "Now upload the source distribution with training application to Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cff6c01-311d-4a97-961e-440ea07b9e63",
   "metadata": {
    "id": "a3492f5366aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://python_package/dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  6.4 KiB/  6.4 KiB]                                                \n",
      "Operation completed over 1 objects/6.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {source_package_file_name} {python_package_gcs_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a3eb2-e1a6-4b4d-b3d2-b40b0a248a0c",
   "metadata": {
    "id": "1e4a3c5a11a1"
   },
   "source": [
    "Validate the source distribution exists on Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "769da3fb-7444-4c73-a1f5-690e048f314b",
   "metadata": {
    "id": "29fd8224d9cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6529  2022-03-26T18:04:35Z  gs://gcp-ml-projects-bucket/pytorch-on-gcp/testing-mt5-summ-es/train/python_package/trainer-0.1.tar.gz\n",
      "TOTAL: 1 objects, 6529 bytes (6.38 KiB)\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -l {python_package_gcs_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dbfc17-f962-42ec-ba75-abc9de2c43df",
   "metadata": {
    "id": "4a1a4281ff7f"
   },
   "source": [
    "#### **Run custom training job on Vertex AI**\n",
    "\n",
    "We use [Vertex AI SDK for Python](https://cloud.google.com/vertex-ai/docs/start/client-libraries#client_libraries) to create and submit training job to the Vertex AI training service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f5395-e63a-415f-a819-d9b21445ac59",
   "metadata": {
    "id": "5d2957ef04fd"
   },
   "source": [
    "##### **Initialize the Vertex AI SDK for Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "845dacef-dc6c-4f14-8252-af454a13892f",
   "metadata": {
    "id": "c4a578b55943"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7942ad0-c8e0-443f-a5eb-6f45f3ccd378",
   "metadata": {
    "id": "6b0fed34b728"
   },
   "source": [
    "##### **Configure and submit Custom Job to Vertex AI Training service**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57bacc-e7e5-4d66-be86-8ef7b1c7c68c",
   "metadata": {
    "id": "8f24d5bdc31d"
   },
   "source": [
    "Configure a [Custom Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job) with the [pre-built container](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) image for PyTorch and training code packaged as Python source distribution. \n",
    "\n",
    "**NOTE:** When using Vertex AI SDK for Python for submitting a training job, it creates a [Training Pipeline](https://cloud.google.com/vertex-ai/docs/training/create-training-pipeline) which launches the Custom Job on Vertex AI Training service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "957410f0-4391-4e7d-a4fa-ecd3e09e75c8",
   "metadata": {
    "id": "daababed95ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APP_NAME=testing-mt5-summ-es\n",
      "PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-7:latest\n",
      "python_package_gcs_uri=gs://gcp-ml-projects-bucket/pytorch-on-gcp/testing-mt5-summ-es/train/python_package/trainer-0.1.tar.gz\n",
      "python_module_name=trainer.task\n"
     ]
    }
   ],
   "source": [
    "print(f\"APP_NAME={APP_NAME}\")\n",
    "print(\n",
    "    f\"PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI={PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI}\"\n",
    ")\n",
    "print(f\"python_package_gcs_uri={python_package_gcs_uri}\")\n",
    "print(f\"python_module_name={python_module_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b673803e-8fb5-44d6-b5ae-ae8001b54786",
   "metadata": {
    "id": "1f7cff892eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME=testing-mt5-summ-es-pytorch-pkg-ar-20220326183026\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = f\"{APP_NAME}-pytorch-pkg-ar-{get_timestamp()}\"\n",
    "print(f\"JOB_NAME={JOB_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c8468",
   "metadata": {},
   "source": [
    "Create a Python package training job to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c2c882e-32af-4831-9eb1-73f4cae934ad",
   "metadata": {
    "id": "e7f68fec44ed"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomPythonPackageTrainingJob(\n",
    "    display_name=f\"{JOB_NAME}\",\n",
    "    python_package_gcs_uri=python_package_gcs_uri,\n",
    "    python_module_name=python_module_name,\n",
    "    container_uri=PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1b10c",
   "metadata": {},
   "source": [
    "Next we set the training arguments depends on the initial pretrained model we choose. We have selected a mBART and mT5 model as our initial models, so we should run the next cell based on the initial model we want to finetune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b52f48d-b77a-4781-818e-85c314901595",
   "metadata": {
    "id": "aaeb607c356e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://gcp-ml-projects-bucket/aiplatform-custom-training-2022-03-11-23:00:26.528 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7738873559438589952?project=42794247013\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/7738873559438589952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4280599977559851008?project=42794247013\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/7738873559438589952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/7738873559438589952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/7738873559438589952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/7738873559438589952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/7738873559438589952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/7738873559438589952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/7738873559438589952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/7738873559438589952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/7738873559438589952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "# mBART training arguments\n",
    "training_args = [\"--epochs\", \"5\", \"--model-name\", \"mrm8488/mbart-large-finetuned-opus-en-es-translation\",\"--dataset_name\",\"mlsum\",\"--train_split\",\"train\",\"--val_split\",\"validation\",\"--test_split\",\"test\",\n",
    "                \"--trained-model-name\", \"mbart-large-finetune-mlsum-es\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87275ee1-9c1a-45e7-a2f0-70c84daa43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mT5 training arguments\n",
    "training_args = [\"--epochs\", \"2\", \"--model-name\", \"google/mt5-small\", \"--dataset_name\",\"mlsum\",\n",
    "                 \"--train_split\",\"train[:3%]\",\"--val_split\",\"validation[:5%]\",\"--test_split\",\"test[:5%]\",\n",
    "                 \"--lr\", \"1e-5\", \"--warmup_steps\", \"100\", \"--train_batch_size\", \"8\", \n",
    "                 \"--wandb-api-key\", \"no-logging\",\"--push_to_hub\",\"n\",\n",
    "                \"--trained-model-name\", \"mT5-base-finetune-mlsum-es\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4690b438-e4f6-4b90-bd0e-ed4e9a83c573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://gcp-ml-projects-bucket/aiplatform-custom-training-2022-03-26-18:30:45.169 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6643786366452760576?project=42794247013\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/6643786366452760576 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6700679496120401920?project=42794247013\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/6643786366452760576 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/6643786366452760576 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/6643786366452760576 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/6643786366452760576 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/6643786366452760576 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/6643786366452760576 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/6643786366452760576 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob run completed. Resource name: projects/42794247013/locations/us-central1/trainingPipelines/6643786366452760576\n",
      "WARNING:google.cloud.aiplatform.training_jobs:Training did not produce a Managed Model returning None. Training Pipeline projects/42794247013/locations/us-central1/trainingPipelines/6643786366452760576 is not configured to upload a Model. Create the Training Pipeline with model_serving_container_image_uri and model_display_name passed in. Ensure that your training script saves to model to os.environ['AIP_MODEL_DIR'].\n"
     ]
    }
   ],
   "source": [
    "model = job.run(\n",
    "    replica_count=1,\n",
    "    # For mBART training\n",
    "    #machine_type=\"a2-highgpu-1g\",\n",
    "    #accelerator_type=\"NVIDIA_TESLA_A100\",\n",
    "    # For mT5 training\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_V100\",\n",
    "    accelerator_count=1,\n",
    "    args=training_args,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b885e-78ec-439d-a76f-6c58ac41188e",
   "metadata": {
    "id": "760ab4b88042"
   },
   "source": [
    "Validate the model artifacts written to GCS by the training code after the job completes successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ecbed4e6-1422-4e1e-9ff5-56aa3e4f3635",
   "metadata": {
    "id": "39570bd8def9"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_pb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10567/1815383163.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjob_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMessageToDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m gcs_model_artifacts_uri = job_response[\"trainingTaskInputs\"][\"baseOutputDirectory\"][\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"outputUriPrefix\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model artifacts are available at {gcs_model_artifacts_uri}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_pb'"
     ]
    }
   ],
   "source": [
    "job_response = MessageToDict(job._gca_resource._pb)\n",
    "gcs_model_artifacts_uri = job_response[\"trainingTaskInputs\"][\"baseOutputDirectory\"][\n",
    "    \"outputUriPrefix\"\n",
    "]\n",
    "print(f\"Model artifacts are available at {gcs_model_artifacts_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02930c6b-144a-46a8-9f50-c8c8ab0f3395",
   "metadata": {
    "id": "09eaae731189"
   },
   "outputs": [],
   "source": [
    "!gsutil ls -lr $gcs_model_artifacts_uri/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b379a66e-fea6-4669-89fe-faf16fc3411c",
   "metadata": {
    "id": "c170d386492b"
   },
   "source": [
    "## Run Custom Job on Vertex AI Training with custom container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e65c5-3e04-4779-808d-013e47ce3792",
   "metadata": {
    "id": "035227b6e581"
   },
   "source": [
    "To create a [training job with custom container](https://cloud.google.com/vertex-ai/docs/training/create-custom-container?hl=hr), you define a `Dockerfile` to install or add the dependencies required for the training job. Then, you build and test your Docker image locally to verify, push the image to Container Registry and submit a Custom Job to Vertex AI Training service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d4a24-d22e-40ed-8604-bbdaf0a76f93",
   "metadata": {
    "id": "95ea5367dc42"
   },
   "source": [
    "#### **Build your container using Dockerfile with Training Code and Dependencies**\n",
    "\n",
    "In the previous section, we wrapped the training application code and dependencies as Python source distribution. An alternate way to package the training application and dependencies is to [create a custom container](https://cloud.google.com/vertex-ai/docs/training/create-custom-container?hl=hr) using Dockerfile. We create a Dockerfile with a [pre-built PyTorch container image provided by Vertex AI](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers#available_container_images) as the base image, install the dependencies - `transformers`, `datasets` , `nltk`, `rouge-score`, `wandb` and `cloudml-hypertune` and copy the training application code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9b96b23-c8ef-4af8-9c91-64fa331ae4e2",
   "metadata": {
    "id": "59cf98db5792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./custom_container/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "%%writefile ./custom_container/Dockerfile\n",
    "\n",
    "# Use pytorch GPU base image\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-10:latest\n",
    "\n",
    "# set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install required packages\n",
    "RUN pip install google-cloud-storage transformers datasets nltk rouge-score sentencepiece cloudml-hypertune\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY ./trainer/__init__.py /app/trainer/__init__.py\n",
    "COPY ./trainer/experiment.py /app/trainer/experiment.py\n",
    "COPY ./trainer/utils.py /app/trainer/utils.py\n",
    "COPY ./trainer/task.py /app/trainer/task.py\n",
    "\n",
    "# Set up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d3310a5-9954-4586-941e-1b02fbf21afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_container  scripts\t\t\t       train.py\n",
      "python_package\t  summarization_mlsum_es_vertex.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfeebf8-30fc-4a02-be6b-4580706af94b",
   "metadata": {},
   "source": [
    "**If your custom container was built previously just run the next cell, if not run the cell that run the `docker build`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7856d5ab-aa77-4937-98dc-84c62d93f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_TRAIN_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_gpu_train_{APP_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f139b-0ccc-45fa-8371-b2ee3350fdae",
   "metadata": {
    "id": "1f8d15e545e7"
   },
   "source": [
    "Build the image and tag the Container Registry path (gcr.io) that you will push to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7451e3ce-b7c0-45df-bfd1-86f32a7af06e",
   "metadata": {
    "id": "99d59a1f072c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  64.05kB\n",
      "Step 1/9 : FROM us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-10:latest\n",
      " ---> 3210fbe7551d\n",
      "Step 2/9 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> 2e0346acbd78\n",
      "Step 3/9 : RUN pip install google-cloud-storage transformers==4.17.0 datasets==2.0.0 nltk rouge-score sentencepiece cloudml-hypertune wandb==0.12.11\n",
      " ---> Using cache\n",
      " ---> dc4c79fd467e\n",
      "Step 4/9 : RUN   apt-get update &&   apt-get install -y sudo curl git &&   curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash &&   sudo apt-get install git-lfs=3.1.2 && git lfs install\n",
      " ---> Using cache\n",
      " ---> a239344213d0\n",
      "Step 5/9 : COPY ./trainer/__init__.py /app/trainer/__init__.py\n",
      " ---> Using cache\n",
      " ---> 78dcc1c8dda9\n",
      "Step 6/9 : COPY ./trainer/experiment.py /app/trainer/experiment.py\n",
      " ---> 220d04dd68c0\n",
      "Step 7/9 : COPY ./trainer/utils.py /app/trainer/utils.py\n",
      " ---> 377dff866431\n",
      "Step 8/9 : COPY ./trainer/task.py /app/trainer/task.py\n",
      " ---> 07ba6eefdabc\n",
      "Step 9/9 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in e90544b9740a\n",
      "Removing intermediate container e90544b9740a\n",
      " ---> 367a2f8a4c4a\n",
      "Successfully built 367a2f8a4c4a\n",
      "Successfully tagged gcr.io/gcp-ml-projects/pytorch_gpu_train_mt5-small-summ-mlsum-es:latest\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_TRAIN_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_gpu_train_{APP_NAME}\"\n",
    "\n",
    "!cd ./custom_container/ && docker build -f Dockerfile -t $CUSTOM_TRAIN_IMAGE_URI ../python_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b301830c-b3cd-4878-9e97-ecba26840e84",
   "metadata": {
    "id": "d5e11e1319b6"
   },
   "source": [
    "##### **Push the container to Container Registry**\n",
    "\n",
    "Push your container image with training application code and dependencies to your Container Registry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ad6f88f-f090-4a18-8f9d-1ccd1bd3a0a9",
   "metadata": {
    "id": "11fbe35013f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/gcp-ml-projects/pytorch_gpu_train_mt5-small-summ-mlsum-es]\n",
      "\n",
      "\u001b[1B574c2d82: Preparing \n",
      "\u001b[1Bb91ef80c: Preparing \n",
      "\u001b[1Bdbb179c6: Preparing \n",
      "\u001b[1B1ac12390: Preparing \n",
      "\u001b[1Be2d9c00c: Preparing \n",
      "\u001b[1B214b3e27: Preparing \n",
      "\u001b[1Be421f4be: Preparing \n",
      "\u001b[1B01fd3550: Preparing \n",
      "\u001b[1B105d38de: Preparing \n",
      "\u001b[1B6867eca5: Preparing \n",
      "\u001b[1Beb8da3b6: Preparing \n",
      "\u001b[1B482e0a93: Preparing \n",
      "\u001b[1Bbfc9e711: Preparing \n",
      "\u001b[1B1e557a4a: Preparing \n",
      "\u001b[1B4a3ae7b0: Preparing \n",
      "\u001b[1B6f2be7a4: Preparing \n",
      "\u001b[1B744e683f: Preparing \n",
      "\u001b[1B615e3baa: Preparing \n",
      "\u001b[1Bd6391439: Preparing \n",
      "\u001b[15B14b3e27: Waiting g \n",
      "\u001b[1Bef90a537: Preparing \n",
      "\u001b[16B421f4be: Waiting g \n",
      "\u001b[1Bb915feb5: Preparing \n",
      "\u001b[1B7c56669b: Preparing \n",
      "\u001b[1B52b2be28: Preparing \n",
      "\u001b[1B5a3d56c5: Preparing \n",
      "\u001b[19B05d38de: Waiting g \n",
      "\u001b[1B160a6176: Preparing \n",
      "\u001b[1Bfa8c5974: Preparing \n",
      "\u001b[21B867eca5: Waiting g \n",
      "\u001b[1B3a5a2020: Preparing \n",
      "\u001b[22Bb8da3b6: Waiting g \n",
      "\u001b[1B8ec8f441: Preparing \n",
      "\u001b[23B82e0a93: Waiting g \n",
      "\u001b[1Bea620f67: Preparing \n",
      "\u001b[19B15e3baa: Waiting g \n",
      "\u001b[25Bfc9e711: Waiting g \n",
      "\u001b[1B33725290: Preparing \n",
      "\u001b[21B6391439: Waiting g \n",
      "\u001b[27Be557a4a: Waiting g \n",
      "\u001b[1B28689930: Preparing \n",
      "\u001b[28Ba3ae7b0: Waiting g \n",
      "\u001b[1B3d846799: Preparing \n",
      "\u001b[29Bf2be7a4: Waiting g \n",
      "\u001b[1B88464538: Preparing \n",
      "\u001b[25B6f84f46: Waiting g \n",
      "\u001b[31B44e683f: Waiting g \n",
      "\u001b[25Bc56669b: Waiting g \n",
      "\u001b[24Ba3d56c5: Waiting g \n",
      "\u001b[14B813689f: Waiting g \n",
      "\u001b[1Ba1abcbfa: Layer already exists 8kB\u001b[47A\u001b[2K\u001b[46A\u001b[2K\u001b[45A\u001b[2K\u001b[44A\u001b[2K\u001b[42A\u001b[2K\u001b[41A\u001b[2K\u001b[40A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[34A\u001b[2K\u001b[33A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[17A\u001b[2K\u001b[15A\u001b[2K\u001b[51A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2Klatest: digest: sha256:baa6099c4e4a27af1d68f32a995e57fafc5b7ba1e585715c685787cfaa8b22d6 size: 11194\n"
     ]
    }
   ],
   "source": [
    "!docker push $CUSTOM_TRAIN_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a136b357-cf2d-45b0-a38a-e71d571579ac",
   "metadata": {
    "id": "0d157a91a792"
   },
   "source": [
    "Validate the custom container image in Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f533ddba-d548-4425-a71a-a359fe6f2b1d",
   "metadata": {
    "id": "a23a4c447117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Successfully resolved tag to sha256, but it is recommended to use sha256 directly.\n",
      "image_summary:\n",
      "  digest: sha256:baa6099c4e4a27af1d68f32a995e57fafc5b7ba1e585715c685787cfaa8b22d6\n",
      "  fully_qualified_digest: gcr.io/gcp-ml-projects/pytorch_gpu_train_mt5-small-summ-mlsum-es@sha256:baa6099c4e4a27af1d68f32a995e57fafc5b7ba1e585715c685787cfaa8b22d6\n",
      "  registry: gcr.io\n",
      "  repository: gcp-ml-projects/pytorch_gpu_train_mt5-small-summ-mlsum-es\n"
     ]
    }
   ],
   "source": [
    "!gcloud container images describe $CUSTOM_TRAIN_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0eacc9-bfb7-4ae1-903e-7b00b4f3da34",
   "metadata": {
    "id": "a23e5e34bea9"
   },
   "source": [
    "##### **Initialize the Vertex AI SDK for Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f517c875-be92-4e4b-8f74-7cdbd0ab248c",
   "metadata": {
    "id": "52f180952ca0"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31fb3427-4fb5-449a-9c20-a22e23bbcdfd",
   "metadata": {
    "id": "e5e048cc0500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APP_NAME=mt5-small-summ-mlsum-es\n",
      "CUSTOM_TRAIN_IMAGE_URI=gcr.io/gcp-ml-projects/pytorch_gpu_train_mt5-small-summ-mlsum-es\n",
      "JOB_NAME=mt5-small-summ-mlsum-es-pytorch-cstm-cntr-20220327160104\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = f\"{APP_NAME}-pytorch-cstm-cntr-{get_timestamp()}\"\n",
    "\n",
    "print(f\"APP_NAME={APP_NAME}\")\n",
    "print(f\"CUSTOM_TRAIN_IMAGE_URI={CUSTOM_TRAIN_IMAGE_URI}\")\n",
    "print(f\"JOB_NAME={JOB_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447232fb-2264-499a-aed6-e95b46fb465c",
   "metadata": {},
   "source": [
    "### Get the Weight & biases API key to log training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e79117-c84e-4ff4-92dd-a45240370f72",
   "metadata": {},
   "source": [
    "The W&B API key should be store in a txt file called `secrets.txt`, the first line in the text file. **This file must not be uploaded to your source code repository**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9a4e7df-4085-4649-9ded-8e6180e11877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the txt file with the wandb API Key\n",
    "path_to_wandb_api=\"secrets.txt\"\n",
    "# Read the APi Key, if not we set the no-logging strategy\n",
    "try:\n",
    "    with open(path_to_wandb_api) as f:\n",
    "        mywandb_api_key = f.readline()\n",
    "except:\n",
    "    mywandb_api_key=\"no-logging\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ae4bb0-b9fd-4ba8-84de-7a87e02af57d",
   "metadata": {},
   "source": [
    "### Log in to the Huggingface account and get the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7c9fb86-9ba7-4f5a-aecc-5c9003a51624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your password: ············\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "hf_username = \"edumunozsala\" # your username on huggingface.co\n",
    "hf_email = \"edumunozsala@gmail.com\" # email used for commit\n",
    "# Define the final model name\n",
    "mymodel_name='mT5-small-finetune-mlsum-es'\n",
    "repository_name = mymodel_name\n",
    "password = getpass(\"Enter your password:\") # creates a prompt for entering password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f77af733-267d-4a42-bdf3-17fb63aae0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:root:HfApi.login: This method is deprecated in favor of `set_access_token`.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, Repository\n",
    "\n",
    "# get hf token\n",
    "token = HfApi().login(username=hf_username, password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a89d1f4-a3ac-48b4-8318-03cf76d4e225",
   "metadata": {
    "id": "b7a24d15e8e9"
   },
   "outputs": [],
   "source": [
    "# define training code arguments\n",
    "training_args = [\"--epochs\", \"5\", \"--model-name\", \"google/mt5-small\", \"--dataset_name\",\"mlsum\",\n",
    "                 \"--train_split\",\"train\",\"--val_split\",\"validation\",\"--test_split\",\"test\",\n",
    "                 \"--lr\", \"1e-5\", \"--warmup_steps\", \"1000\", \"--weight-decay\", \"0.01\", \"--train_batch_size\", \"8\", \n",
    "                 \"--max_input_length\", \"512\", \"--max_target_length\", \"64\",\n",
    "                 \"--wandb-api-key\", mywandb_api_key,\"--push_to_hub\",\"y\",\"--hub_model_id\", \"mT5-small-finetune-mlsum-es\", \"--hub_token\", token,\n",
    "                \"--trained-model-name\", \"mT5-small-finetune-mlsum-es\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dec651a-67c0-4391-85b7-e7331c53ac84",
   "metadata": {
    "id": "abf1fa4085cb"
   },
   "source": [
    "##### **Configure and submit Custom Job to Vertex AI Training service**\n",
    "\n",
    "Configure a [Custom Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job) with the [custom container](https://cloud.google.com/vertex-ai/docs/training/create-custom-container) image with training code and other dependencies\n",
    "\n",
    "**NOTE:** When using Vertex AI SDK for Python for submitting a training job, it creates a [Training Pipeline](https://cloud.google.com/vertex-ai/docs/training/create-training-pipeline) which launches the Custom Job to train on Vertex AI Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a782ece-62c0-4c94-bd12-11d6be242c9c",
   "metadata": {
    "id": "6946d8ad77f6"
   },
   "outputs": [],
   "source": [
    "# configure the job with container image spec\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=f\"{JOB_NAME}\", container_uri=f\"{CUSTOM_TRAIN_IMAGE_URI}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b632e4d-f154-4bc9-ada3-a4a69aefb34d",
   "metadata": {
    "id": "7459d3261450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://gcp-ml-projects-bucket/aiplatform-custom-training-2022-03-27-16:11:24.559 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2922827911826243584?project=42794247013\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/2922827911826243584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4684227947337351168?project=42794247013\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/2922827911826243584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/2922827911826243584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/2922827911826243584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/2922827911826243584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/2922827911826243584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/2922827911826243584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/42794247013/locations/us-central1/trainingPipelines/2922827911826243584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "# submit the custom job to Vertex AI training service\n",
    "model = job.run(\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_V100\",\n",
    "    accelerator_count=1,\n",
    "    args=training_args,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b306c3-1229-43d2-b671-ae7f44168bd8",
   "metadata": {
    "id": "409c7472fd4f"
   },
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066636c-aafa-46e2-9856-458610991c9f",
   "metadata": {
    "id": "ba6122f929e3"
   },
   "source": [
    "The training application code for fine-tuning a transformer model for sentiment analysis task uses hyperparameters such as learning rate and weight decay. These hyperparameters control the behavior of the training algorithm and can have a significant effect on the performance of the resulting model. This part of the notebook show how you can automate tuning these hyperparameters with Vertex AI Training service.\n",
    "\n",
    "We submit a [Hyperparameter Tuning job](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) to Vertex AI Training service by packaging the training application code and dependencies in a Docker container and push the container to Google Container Registry, similar to running a Custom Job on Vertex AI with Custom Container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18de6e-be49-47c9-b3ea-40530e634eda",
   "metadata": {
    "id": "e1570be63a7c"
   },
   "source": [
    "### How hyperparameter tuning works in Vertex AI?\n",
    "\n",
    "Following are the high level steps involved in running a Hyperparameter Tuning job on Vertex AI Training service:\n",
    "\n",
    "- You define the hyperparameters to tune the model along with the metric (or goal) to optimize\n",
    "- Vertex AI runs multiple trials of your training application with the hyperparameters and limits you specified - maximum number of trials to run and number of parallel trials. \n",
    "- Vertex AI keeps track of the results from each trial and makes adjustments for subsequent trials. This requires your training application to report the metrics to Vertex AI using the Python package [`cloudml-hypertune`](https://github.com/GoogleCloudPlatform/cloudml-hypertune). \n",
    "- When the job is finished, get the summary of all the trials with the most effective configuration of values based on the criteria you configured\n",
    "\n",
    "Refer to the [Vertex AI documentation](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) to understand how to configure and select hyperparameters for tuning, configure tuning strategy and how Vertex AI optimizes the hyperparameter tuning jobs. The default tuning strategy uses results of previous trials to inform the assignment of values in subsequent trials. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9add42-17c9-495d-b4c3-73070a508b64",
   "metadata": {
    "id": "d4dc37f4aab3"
   },
   "source": [
    "### Changes to training application code for hyperparameter tuning\n",
    "\n",
    "There are few [requirements](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview#the_flow_of_hyperparameter_values) to follow specific to hyperparameter tuning in Vertex AI:\n",
    "\n",
    "1. To pass the hyperparameter values to training code, you mist define a command-line argument in the main training module for each tuned hyperparameter. Use the value passed in those arguments to set the corresponding hyperparameter in the training application's code\n",
    "1. You must pass metrics from the training application to Vertex AI to evaluate the effectiveness of a trial. You can use [`cloudml-hypertune` Python package](https://github.com/GoogleCloudPlatform/cloudml-hypertune) to report metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60173b1-08d5-4ef4-b857-29ef024d5e03",
   "metadata": {
    "id": "4b0331ad2e11"
   },
   "source": [
    "Previously, in the training application code to fine-tune the transformer model for sentiment analysis task, we instantiated [`Trainer`](https://huggingface.co/transformers/main_classes/trainer.html) with hyperparameters passed as training arguments (`training_args`). These hyperparameters are passed as command line arguments to the training module `trainer.task` which are then passed to the `training_args`. Refer to `./python_package/trainer` module for training application code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c3e860-c8bc-4ff5-95ff-344277719245",
   "metadata": {
    "id": "16fb8c85a170"
   },
   "source": [
    "To report metrics to Vertex AI when hyperparameter tuning is enabled, we call [`cloudml-hypertune` Python package](https://github.com/GoogleCloudPlatform/cloudml-hypertune) after the evaluation phase which is added as a [callback](https://huggingface.co/transformers/main_classes/callback.html#transformers.trainer_callback.TrainerCallback) to the `trainer`. The `trainer` objects passes the metrics computed by the last evaluation phase to the callback which will be reported by `hypertune` library to Vertex AI for evaluating trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dbca71-c5b1-4c1e-977a-26b182bb57af",
   "metadata": {
    "id": "a9d284f3599d"
   },
   "source": [
    "### Run Hyperparameter Tuning Job on Vertex AI\n",
    "\n",
    "Before submitting the hyperparameter tuning job to Vertex AI, push the custom container image with training application to Google Cloud Container Registry and then submit the job to Vertex AI. We will be using the same image used for running Custom Job on Vertex AI Training service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9942bf-03fe-4361-adb8-7637e16e41e4",
   "metadata": {
    "id": "0ab9c7321d2f"
   },
   "source": [
    "Validate the custom container image in Container Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb5e13b-8dab-417c-b1f0-8b07bf8e97e7",
   "metadata": {
    "id": "2bdb8c21d0d5"
   },
   "source": [
    "Following is the `setup.py` file for the training application. The `find_packages()` function inside `setup.py` includes the `trainer` directory in the package as it contains `__init__.py` which tells [Python Setuptools](https://setuptools.readthedocs.io/en/latest/) to include all subdirectories of the parent directory as dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f9156-fa68-406f-a27a-19b9cdbf90b2",
   "metadata": {
    "id": "f60fab07d67c"
   },
   "source": [
    "##### **Initialize the Vertex AI SDK for Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cafc9ea2-366c-434e-ab5e-d2b841a200e6",
   "metadata": {
    "id": "7f914ea43ac0"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26655df3-056d-4315-af43-ae42f3e8a77c",
   "metadata": {
    "id": "6652aa63ddff"
   },
   "source": [
    "##### **Configure and submit Hyperparameter Tuning Job to Vertex AI Training service**\n",
    "\n",
    "Configure a [Hyperparameter Tuning Job](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning) with the [custom container](https://cloud.google.com/vertex-ai/docs/training/create-custom-container) image with training code and other dependencies.\n",
    "\n",
    "When configuring and submitting a Hyperparameter Tuning job, you need to attach a Custom Job definition with worker pool specs defining machine type, accelerators and URI for container image representing the custom container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a2206e2-0ae4-4752-b56b-d782c0e2537c",
   "metadata": {
    "id": "93da68249c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APP_NAME=finetuned-mt5-summ-es\n",
      "CUSTOM_TRAIN_IMAGE_URI=gcr.io/gcp-ml-projects/pytorch_gpu_train_finetuned-mt5-summ-es\n",
      "JOB_NAME=finetuned-mt5-summ-es-pytorch-hptune-20220315141748\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = f\"{APP_NAME}-pytorch-hptune-{get_timestamp()}\"\n",
    "\n",
    "print(f\"APP_NAME={APP_NAME}\")\n",
    "print(f\"CUSTOM_TRAIN_IMAGE_URI={CUSTOM_TRAIN_IMAGE_URI}\")\n",
    "#print(f\"PRE_BUILT_TRAIN_IMAGE_URI={PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI}\") \n",
    "print(f\"JOB_NAME={JOB_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5e8fe-2c83-42da-a635-a2e2bb2a41f4",
   "metadata": {
    "id": "9d46db3a8b23"
   },
   "source": [
    "Define the training arguments with `hp-tune` argument set to `y` so that training application code can report metrics to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b3b781b-1e9f-4ff6-9d1a-6fffa001b80d",
   "metadata": {
    "id": "30b206352f6e"
   },
   "outputs": [],
   "source": [
    "training_args = [\"--epochs\", \"4\", \"--model-name\", \"google/mt5-small\",\"--dataset_name\",\"mlsum\",\"--train_split\",\"train[:15%]\",\"--val_split\",\"validation[:50%]\",\"--test_split\",\"test\",\n",
    "                \"--trained-model-name\", \"mt5-small-finetune-mlsum-es\",\"--hp-tune\", \"y\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ccfa7-2be1-4364-a661-2ef8ab9d9f92",
   "metadata": {
    "id": "0e84606af6ab"
   },
   "source": [
    "Create a **`CustomJob`** with worker pool specs to define machine types, accelerators and customer container spec with the training application code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e84637f5-214f-474d-b0b3-f729aec0feb9",
   "metadata": {
    "id": "0456de4efb46"
   },
   "outputs": [],
   "source": [
    "# The spec of the worker pools including machine type and Docker image\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-8\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "            \"accelerator_count\": 2,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\"image_uri\": CUSTOM_TRAIN_IMAGE_URI, \"args\": training_args},\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17a95fe8-6a39-46cf-a54d-199de278213f",
   "metadata": {
    "id": "1339a33f3c3c"
   },
   "outputs": [],
   "source": [
    "custom_job = aiplatform.CustomJob(\n",
    "    display_name=JOB_NAME, worker_pool_specs=worker_pool_specs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8c857-e45c-4688-961c-9164587e6814",
   "metadata": {
    "id": "269d6ffa999f"
   },
   "source": [
    "Define the `parameter_spec` as a Python dictionary object with the search space i.e. parameters to search and optimize. They key is the hyperparameter name passed as command line argument to the training code and value is the parameter specification. The spec requires to specify the hyperparameter data type as an instance of a parameter value specification. \n",
    "\n",
    "Refer to the [documentation](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview#hyperparameters) on selecting the hyperparaneter to tune and how to define parameter specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "456e6036-227d-4e55-bedf-2c1d32478cd2",
   "metadata": {
    "id": "4210677c7ec7"
   },
   "outputs": [],
   "source": [
    "# Dictionary representing parameters to optimize.\n",
    "# The dictionary key is the parameter_id, which is passed into your training\n",
    "# job as a command line argument,\n",
    "# And the dictionary value is the parameter specification of the metric.\n",
    "parameter_spec = {\n",
    "    \"lr\": hpt.DoubleParameterSpec(min=1e-7, max=5e-4, scale=\"log\"),\n",
    "    \"weight-decay\": hpt.DiscreteParameterSpec(\n",
    "        values=[0.001, 0.01], scale=None\n",
    "    ),\n",
    "    \"train_batch_size\": hpt.DiscreteParameterSpec(\n",
    "        values=[2, 4, 8], scale=None\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c5d7b-5022-4f00-9089-1a7e5be434e5",
   "metadata": {
    "id": "e29058ada6d4"
   },
   "source": [
    "Define the `metric_spec` with name and goal of metric to optimize. The goal specifies whether you want to tune your model to maximize or minimize the value of this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bb960ca-4e09-4c3c-a4de-1837bfc993e3",
   "metadata": {
    "id": "6d50f78b1303"
   },
   "outputs": [],
   "source": [
    "# Dictionary representing metrics to optimize.\n",
    "# The dictionary key is the metric_id, which is reported by your training job,\n",
    "# And the dictionary value is the optimization goal of the metric.\n",
    "metric_spec = {\"rouge2\": \"maximize\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2b65f-45f7-4d84-8d88-e137a85a9b09",
   "metadata": {
    "id": "3c5215ca590c"
   },
   "source": [
    "Configure and submit a Hyperparameter Tuning Job with the Custom Job, metric spec, parameter spec and trial limits.\n",
    "\n",
    "- **`max_trial_count`**: Maximum # of Trials run by the service. We recommend to start with a smaller value to understand the impact of the hyperparameters chosen before scaling up.\n",
    "- **`parallel_trial_count`**: Number of Trials to run in parallel. We recommend to start with a smaller value as Vertex AI uses results from the previous trials to inform the assignment of values in subsequent trials. Large # of parallel trials mean these trials start without having the benefit of the results of any trials still running.\n",
    "- **`search_algorithm`**: Search algorithm specified for the Study. If you do not specify an algorithm, Vertex AI by default applies Bayesian optimization to arrive at the optimal solution to search over the parameter space. \n",
    "\n",
    "Refer to the [documentation](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#configuration) to understand the hyperparameter training job configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "003cbe5e-215d-46c1-90b9-183a949f5830",
   "metadata": {
    "id": "8d8088bfa3ad"
   },
   "outputs": [],
   "source": [
    "hp_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=JOB_NAME,\n",
    "    custom_job=custom_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=15,\n",
    "    parallel_trial_count=2,\n",
    "    search_algorithm=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5ed6687-d8a4-4720-a93d-daff1b3808b9",
   "metadata": {
    "id": "d4f07757c0a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating HyperparameterTuningJob\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob created. Resource name: projects/42794247013/locations/us-central1/hyperparameterTuningJobs/2753564693799895040\n",
      "INFO:google.cloud.aiplatform.jobs:To use this HyperparameterTuningJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:hpt_job = aiplatform.HyperparameterTuningJob.get('projects/42794247013/locations/us-central1/hyperparameterTuningJobs/2753564693799895040')\n",
      "INFO:google.cloud.aiplatform.jobs:View HyperparameterTuningJob:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2753564693799895040?project=42794247013\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/42794247013/locations/us-central1/hyperparameterTuningJobs/2753564693799895040 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/42794247013/locations/us-central1/hyperparameterTuningJobs/2753564693799895040 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/42794247013/locations/us-central1/hyperparameterTuningJobs/2753564693799895040 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/42794247013/locations/us-central1/hyperparameterTuningJobs/2753564693799895040 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/42794247013/locations/us-central1/hyperparameterTuningJobs/2753564693799895040 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "model = hp_job.run(sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d8d00d-b44c-4d3a-ae6a-10a8a98a3279",
   "metadata": {
    "id": "d07c7cb26ad8"
   },
   "source": [
    "##### **Monitoring progress of the Custom Job**\n",
    "\n",
    "You can monitor the hyperparameter tuning job launched from Cloud Console following the link [here](https://console.cloud.google.com/vertex-ai/training/hyperparameter-tuning-jobs/) or use gcloud CLI command [`gcloud beta ai custom-jobs stream-logs`](https://cloud.google.com/sdk/gcloud/reference/beta/ai/custom-jobs/stream-logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83bf1ae-8c84-436a-9baa-7cb351ef49b7",
   "metadata": {
    "id": "ba934b434f03"
   },
   "source": [
    "After the job is finished, you can view and format the results of the hyperparameter tuning Trials (run by Vertex AI Training service) as a Pandas dataframe"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "393b72a4b6ebdc985529acd6cde114b0d5d1de410afe38058a3b12aa44b2c79b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
